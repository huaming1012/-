{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在txt文件中随机抽取10%作为测试集，剩余90%作为训练集\n",
    "import random\n",
    "from random import randint\n",
    " \n",
    "oldf = open('词性标注@人民日报199801.txt', 'r',encoding='utf-8')    #要被抽取的文件（原文件）\n",
    "trainf = open('traindata.txt', 'w',encoding='utf-8')  #用于存放训练集\n",
    "testf = open('testdata.txt', 'w',encoding='utf-8')   #用于存放测试集\n",
    "ratio = 0.9  #训练集占比\n",
    "\n",
    "lines = oldf.readlines()\n",
    "for line in lines:\n",
    "    if random.random()<ratio: #数据集分割比例\n",
    "        trainf.write(line)  #训练数据集\n",
    "    else:\n",
    "        testf.write(line)  #测试数据集\n",
    "        \n",
    "oldf.close()\n",
    "trainf.close()\n",
    "testf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#处理原始文档得到词典\n",
    "def string_process(x):    #处理字符串\n",
    "    a=re.sub(r'\\d{8}-\\d{2}-\\d{3}-\\d{3}/m|[/a-z！。”“，、——\\[\\]（）：《》……A-Z？]', \"\", x)\n",
    "#正则表达式去除时间和后面部分的字母和其他符号\n",
    "    b=a.replace(\"  \",\" \")\n",
    "    return b.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理训练集得到只有分词结果的训练集数据\n",
    "def file_process():\n",
    "    s=\"\"\n",
    "    with open('traindata.txt','r',encoding='UTF-8') as f:  #处理原始训练集traindata.txt\n",
    "        for line1 in f:\n",
    "            a=string_process(line1)\n",
    "            a=string_process(a)\n",
    "            s+=a+\"\\n\"\n",
    "    f.close()\n",
    "    with open(\"process_1.txt\",'w',encoding='UTF-8') as w: #结果写入process_1.txt\n",
    "        w.write(s)\n",
    " \n",
    "    w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理测试集，先得到只有分词结果的测试集数据\n",
    "def test_file_process(): \n",
    "    s = \"\"\n",
    "    with open('testdata.txt', 'r', encoding='UTF-8') as f:  #处理原始测试集testdata.txt\n",
    "        for line1 in f:\n",
    "            a = string_process(line1)\n",
    "            a = string_process(a)\n",
    "            s += a + \"\\n\"\n",
    "    f.close()\n",
    "    with open(\"test_process_1.txt\", 'w', encoding='UTF-8') as w:  #结果写入test_process_1.txt\n",
    "        w.write(s)\n",
    " \n",
    "    w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理分词测试集，去掉空格以用于测试模型分词效果\n",
    "def delete_space():\n",
    "    f = open('test_process_1.txt','r', encoding='utf-8')\n",
    "    f2 = open('test_endfile.txt', 'w', encoding='utf-8')\n",
    "    for line in f.readlines():  #每行都去掉空格分隔，直接连成句，写入test_endfile.txt\n",
    "        r = ''.join(line.split()) \n",
    "        r += \"\\n\"\n",
    "        f2.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "#处理训练集和测试集\n",
    "file_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_process()\n",
    "delete_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上处理过后，最终得到的process_1.txt存放分好词的训练集结果，用于生成词典和进行训练；test_endfile.txt存放去掉空格的测试集结果，用于测试分词模型的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用分好词的训练集，生成对应的词典库\n",
    "def get_dic():\n",
    "    words = set() #词典集合，利用集合自动去重的特性\n",
    "    with open('process_1.txt','r',encoding='utf-8') as f:\n",
    "        for line in f.readlines():  #对每行内容提取所有词语\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = line.split()\n",
    "            lineword = set(line)\n",
    "            words = words.union(lineword)  #不断取并集，形成词典\n",
    "    worddict = list(words)\n",
    "    return worddict\n",
    "dic = get_dic() #生成词典dic，用于FMM和BMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FMM分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#FMM即为正向最大匹配，需要从左向右扫描寻找词的最大匹配，从所有词最大的长度开始在词库中进行匹配，直到匹配成功\n",
    "def readfile():\n",
    "    max_length=0\n",
    "    for i in dic:\n",
    "        max_length=max(max_length,len(i))#获得最大长度\n",
    "        \n",
    "    inf=open(\"test_endfile.txt\",'r',encoding='utf-8')\n",
    "    outf=open(\"FMM分词.txt\",'w',encoding='utf-8') #储存分词结果\n",
    "    \n",
    "    lines=inf.readlines()\n",
    "    for line in lines:#分别对每一行进行正向最大匹配处理，即从最长的长度开始逐次递减尝试直至匹配成功\n",
    "        my_list=[]\n",
    "        len_hang=len(line)\n",
    "        while len_hang>0:\n",
    "            tmp=line[0:max_length] #切割字符串\n",
    "            while tmp not in dic:\n",
    "                if len(tmp)==1: #长度为1的时候就直接退出\n",
    "                    break;\n",
    "                tmp=tmp[0:len(tmp)-1]\n",
    "            my_list.append(tmp)\n",
    "            line=line[len(tmp):]\n",
    "            len_hang=len(line)\n",
    "        for i in my_list:\n",
    "            outf.write(i+\"/\")#用/来分隔分词结果\n",
    "            \n",
    "    outf.close()\n",
    "    inf.close()\n",
    "    \n",
    "    \n",
    "readfile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMM分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#BMM即为逆向最大匹配，需要从右向左扫描寻找词的最大匹配，从所有词最大的长度开始在词库中进行匹配，直到匹配成功\n",
    "def readfile():\n",
    "    max_length=0\n",
    "    for i in dic:\n",
    "        max_length=max(max_length,len(i))#获得最大长度\n",
    "        \n",
    "    inf=open(\"test_endfile.txt\",'r',encoding='utf-8')\n",
    "    outf=open(\"BMM分词.txt\",'w',encoding='utf-8') #储存分词结果\n",
    "    \n",
    "    lines=inf.readlines()\n",
    "    for line in lines:#分别对每一行进行逆向最大匹配处理\n",
    "        my_list = []\n",
    "        len_hang=len(line)\n",
    "        while len_hang>0:\n",
    "            tmp=line[max(0,len_hang-max_length):len_hang]#取max防止超出范围\n",
    "            while tmp not in dic:\n",
    "                if len(tmp)==1:\n",
    "                    break;\n",
    "                tmp=tmp[1:len(tmp)]\n",
    "            my_list.append(tmp)\n",
    "            line=line[0:len_hang-len(tmp)]\n",
    "            len_hang=len(line)\n",
    "        while len(my_list):\n",
    "            tt=my_list.pop()#这里类似栈的操作，因为是逆向进行的匹配所以输出顺序要反过来\n",
    "            outf.write(tt+\"/\") #用/来分隔分词结果\n",
    "            \n",
    "    outf.close()\n",
    "    inf.close()\n",
    "    \n",
    "    \n",
    "readfile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最短路径分词法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造词频词典\n",
    "class WordDictModel:\n",
    "    def __init__(self):\n",
    "        self.word_dict = {}\n",
    "        \n",
    "    def update(self, filename): #更新建立词频词典，即每个词语对应有其出现次数\n",
    "        f = open(filename, \"r\", encoding=\"utf-8\")\n",
    "        for line in f:\n",
    "            words = line.split(\" \")\n",
    "            for word in words:\n",
    "                if self.word_dict.get(word): \n",
    "                    self.word_dict[word] += 1 #如果已经出现过，次数+1\n",
    "                else:\n",
    "                    self.word_dict[word] = 1  #第一次出现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用词频词典，构造有向无环图（DAG图），并利用其进行分词\n",
    "class DAG(WordDictModel):\n",
    "    def build_dag(self, sentence): #构建有向无环图\n",
    "        dag = {}\n",
    "        for start in range(len(sentence)): #对每个字，为以其为开头的所有词语建立结点和连边（权值为词频）\n",
    "            unique = [start + 1] #与start相连的结点在句子中下标\n",
    "            tmp = [(start + 1, 1)] #边及其权重\n",
    "            for stop in range(start+1, len(sentence)+1):\n",
    "                fragment = sentence[start:stop]\n",
    "                num = self.word_dict.get(fragment, 0)\n",
    "                if num > 0 and (stop not in unique): #遇到新的词语，添加连边和权重\n",
    "                    tmp.append((stop, num))\n",
    "                    unique.append(stop)\n",
    "            dag[start] = tmp\n",
    "        return dag\n",
    "    \n",
    "    def predict(self, sentence):  #选择最优路径\n",
    "        Len = len(sentence)\n",
    "        route = [0] * Len\n",
    "        dag = self.build_dag(sentence)  \n",
    "\n",
    "        for i in range(0, Len):\n",
    "            route[i] = max(dag[i], key=lambda x: x[1])[0] #每次都选择权值最大的边所连接的结点作为下一个结点\n",
    "        return route\n",
    "    \n",
    "    def cut(self, sentence):  #按照最优路径来切分原句\n",
    "        route = self.predict(sentence)\n",
    "        next = 0\n",
    "        word_list = []\n",
    "        i = 0\n",
    "        while i < len(sentence):\n",
    "            next = route[i]\n",
    "            word_list.append(sentence[i:next])\n",
    "            i = next\n",
    "        return word_list\n",
    "    \n",
    "    def test(self):\n",
    "        inf=open(\"test_endfile.txt\",'r',encoding='utf-8')\n",
    "        outf=open(\"最短路径分词.txt\",'w',encoding='utf-8')  #储存分词结果\n",
    "        for line in inf.readlines():\n",
    "            result = self.cut(line)\n",
    "            for word in result:\n",
    "                outf.write(word+\"/\")\n",
    "        outf.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#定义DAG图对象，应用于训练集和测试集\n",
    "dag_segger = DAG()\n",
    "dag_segger.update(\"process_1.txt\")\n",
    "dag_segger.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大概率分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大概率分词法需要在词图上选择概率最大的分词路经作为最优结果\n",
    "\n",
    "#首先生成词典，词典中包括词语本身和它出现的概率\n",
    "import math\n",
    "word_dict = {}\n",
    "num_words = 0  #统计总词数\n",
    "f = open(\"process_1.txt\", \"r\", encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    words = line.split(\" \")\n",
    "    for word in words:\n",
    "        num_words += 1  #计算总词数\n",
    "        if word_dict.get(word):\n",
    "            word_dict[word] += 1  #计算每个词的出现次数\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "for word in word_dict:  #所有词的出现次数统计完后，计算其出现概率=出现次数/总词数\n",
    "    #为了防止句子过长导致概率相乘的值太小，在此对概率取对数并加符号，从而把相乘的最大值变成相加的最小值\n",
    "     word_dict[word] = -math.log(word_dict[word] / num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_pro(sentence):\n",
    "    dp = [9999] * len(sentence)  # 到每个字为止的最大概率\n",
    "    root = [0] * len(sentence)  # 每个字所在词的起始下标\n",
    "    max_len = 0\n",
    "    \n",
    "    for i in word_dict.keys():\n",
    "        max_len=max(max_len,len(i))##获得最大长度\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        findflag = 0\n",
    "        for j in range(i, i+max_len):\n",
    "            if j < len(sentence):  #对每个字，尝试以其为开头的所有词\n",
    "                word = sentence[i: j+1]\n",
    "                if word in word_dict.keys():\n",
    "                    findflag = 1\n",
    "                    temp_pro = word_dict[word]\n",
    "                    if i > 0:\n",
    "                        temp_pro += dp[i-1]\n",
    "                    if temp_pro < dp[j]:  #选择保留概率最大的词\n",
    "                        dp[j] = temp_pro\n",
    "                        root[j] = i\n",
    "            else:\n",
    "                break\n",
    "        if (findflag == 0) and (dp[i] == 9999):  #如果以这个字开头的所有词都不在词典中，并且这个字也不在其他词中，则需要单独处理\n",
    "            dp[i] = dp[i - 1] + 20  \n",
    "            root[i] = i  #单字为词，并且代价较大\n",
    "            \n",
    "    # 输出结果\n",
    "    result = []\n",
    "    word_tail = len(sentence) - 1\n",
    "    while word_tail >= 0:  #从词尾开始向前不断寻找最优解\n",
    "        result.append(sentence[root[word_tail]: word_tail + 1])\n",
    "        word_tail = root[word_tail] - 1\n",
    "    result.reverse()\n",
    "    return dp, root, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inf=open(\"test_endfile.txt\",'r',encoding='utf-8')\n",
    "outf=open(\"最大概率分词.txt\",'w',encoding='utf-8') #储存分词结果\n",
    "\n",
    "for line in inf.readlines():\n",
    "    dp, root, result = max_pro(line)\n",
    "    for word in result:\n",
    "        outf.write(word+\"/\")\n",
    "        \n",
    "outf.close()\n",
    "inf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于HMM的分词法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "STATES = ['B', 'M', 'E', 'S']\n",
    "array_A = {}    #状态转移概率矩阵\n",
    "array_B = {}    #发射概率矩阵\n",
    "array_E = {}    #测试集存在的字符，但在训练集中不存在，发射概率矩阵\n",
    "array_Pi = {}   #初始状态分布\n",
    "word_set = set()    #训练数据集中所有字的集合\n",
    "count_dic = {}  #‘B,M,E,S’每个状态在训练集中出现的次数\n",
    "line_num = 0    #训练集语句数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化所有概率矩阵\n",
    "def Init_Array():\n",
    "    for state0 in STATES:\n",
    "        array_A[state0] = {}\n",
    "        for state1 in STATES:\n",
    "            array_A[state0][state1] = 0.0\n",
    "    for state in STATES:\n",
    "        array_Pi[state] = 0.0\n",
    "        array_B[state] = {}\n",
    "        array_E = {}\n",
    "        count_dic[state] = 0\n",
    "\n",
    "#对训练集获取状态标签\n",
    "def get_tag(word):\n",
    "    tag = []\n",
    "    if len(word) == 1:\n",
    "        tag = ['S']\n",
    "    elif len(word) == 2:\n",
    "        tag = ['B', 'E']\n",
    "    else:\n",
    "        num = len(word) - 2\n",
    "        tag.append('B')\n",
    "        tag.extend(['M'] * num)\n",
    "        tag.append('E')\n",
    "    return tag\n",
    "\n",
    "\n",
    "#将参数估计的概率取对数，对概率0取无穷小-3.14e+100\n",
    "def Prob_Array():\n",
    "    for key in array_Pi:\n",
    "        if array_Pi[key] == 0:\n",
    "            array_Pi[key] = -3.14e+100\n",
    "        else:\n",
    "            array_Pi[key] = log(array_Pi[key] / line_num)\n",
    "    for key0 in array_A:\n",
    "        for key1 in array_A[key0]:\n",
    "            if array_A[key0][key1] == 0.0:\n",
    "                array_A[key0][key1] = -3.14e+100\n",
    "            else:\n",
    "                array_A[key0][key1] = log(array_A[key0][key1] / count_dic[key0])\n",
    "    for key in array_B:\n",
    "        for word in array_B[key]:\n",
    "            if array_B[key][word] == 0.0:\n",
    "                array_B[key][word] = -3.14e+100\n",
    "            else:\n",
    "                array_B[key][word] = log(array_B[key][word] /count_dic[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将字典转换成数组\n",
    "def Dic_Array(array_b):\n",
    "    tmp = np.empty((4,len(array_b['B'])))\n",
    "    for i in range(4):\n",
    "        for j in range(len(array_b['B'])):\n",
    "            tmp[i][j] = array_b[STATES[i]][list(word_set)[j]]\n",
    "    return tmp\n",
    "\n",
    "#判断一个字最大发射概率的状态\n",
    "def dist_tag():\n",
    "    array_E['B']['begin'] = 0\n",
    "    array_E['M']['begin'] = -3.14e+100\n",
    "    array_E['E']['begin'] = -3.14e+100\n",
    "    array_E['S']['begin'] = -3.14e+100\n",
    "    array_E['B']['end'] = -3.14e+100\n",
    "    array_E['M']['end'] = -3.14e+100\n",
    "    array_E['E']['end'] = 0\n",
    "    array_E['S']['end'] = -3.14e+100\n",
    "\n",
    "def dist_word(word0,word1,word2,array_b):\n",
    "    if dist_tag(word0,array_b) == 'S':\n",
    "        array_E['B'][word1] = 0\n",
    "        array_E['M'][word1] = -3.14e+100\n",
    "        array_E['E'][word1] = -3.14e+100\n",
    "        array_E['S'][word1] = -3.14e+100\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viterbi算法求测试集的最优状态序列，之后再按照状态序列进行分词\n",
    "def Viterbi(sentence,array_pi,array_a,array_b):\n",
    "    tab = [{}]  #动态规划表\n",
    "    path = {}\n",
    "\n",
    "    if sentence[0] not in array_b['B']:\n",
    "        for state in STATES:\n",
    "            if state == 'S':\n",
    "                array_b[state][sentence[0]] = 0\n",
    "            else:\n",
    "                array_b[state][sentence[0]] = -3.14e+100\n",
    "\n",
    "    for state in STATES:\n",
    "        tab[0][state] = array_pi[state] + array_b[state][sentence[0]]\n",
    "        #tab[t][state]表示时刻t到达state状态的所有路径中，概率最大路径的概率值\n",
    "        path[state] = [state]\n",
    "    for i in range(1,len(sentence)):\n",
    "        tab.append({})\n",
    "        new_path = {}\n",
    "        for state in STATES:\n",
    "            if state == 'B':\n",
    "                array_b[state]['begin'] = 0\n",
    "            else:\n",
    "                array_b[state]['begin'] = -3.14e+100\n",
    "        for state in STATES:\n",
    "            if state == 'E':\n",
    "                array_b[state]['end'] = 0\n",
    "            else:\n",
    "                array_b[state]['end'] = -3.14e+100\n",
    "        for state0 in STATES:\n",
    "            items = []\n",
    "            for state1 in STATES:\n",
    "                if sentence[i] not in array_b[state0]:  #所有在测试集出现但没有在训练集中出现的字符\n",
    "                    if sentence[i-1] not in array_b[state0]:\n",
    "                        prob = tab[i - 1][state1] + array_a[state1][state0] + array_b[state0]['end']\n",
    "                    else:\n",
    "                        prob = tab[i - 1][state1] + array_a[state1][state0] + array_b[state0]['begin']\n",
    "                else:\n",
    "                    prob = tab[i-1][state1] + array_a[state1][state0] + array_b[state0][sentence[i]]    #计算每个字符对应STATES的概率\n",
    "                items.append((prob,state1))\n",
    "            best = max(items)   #bset:(prob,state)保留概率最大的\n",
    "            tab[i][state0] = best[0]\n",
    "            new_path[state0] = path[best[1]] + [state0]\n",
    "        path = new_path\n",
    "\n",
    "    prob, state = max([(tab[len(sentence) - 1][state], state) for state in STATES])\n",
    "    return path[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据状态序列进行分词\n",
    "def tag_seg(sentence,tag):\n",
    "    word_list = []\n",
    "    start = -1\n",
    "    started = False\n",
    "\n",
    "    if len(tag) != len(sentence):\n",
    "        return None\n",
    "\n",
    "    if len(tag) == 1:\n",
    "        word_list.append(sentence[0])   #语句只有一个字，直接输出\n",
    "\n",
    "    else:\n",
    "        if tag[-1] == 'B' or tag[-1] == 'M':    #最后一个字状态不是'S'或'E'则修改\n",
    "            if tag[-2] == 'B' or tag[-2] == 'M':\n",
    "                tag[-1] = 'S'\n",
    "            else:\n",
    "                tag[-1] = 'E'\n",
    "\n",
    "\n",
    "        for i in range(len(tag)):\n",
    "            if tag[i] == 'S':\n",
    "                if started:\n",
    "                    started = False\n",
    "                    word_list.append(sentence[start:i])\n",
    "                word_list.append(sentence[i])\n",
    "            elif tag[i] == 'B':\n",
    "                if started:\n",
    "                    word_list.append(sentence[start:i])\n",
    "                start = i\n",
    "                started = True\n",
    "            elif tag[i] == 'E':\n",
    "                started = False\n",
    "                word = sentence[start:i + 1]\n",
    "                word_list.append(word)\n",
    "            elif tag[i] == 'M':\n",
    "                continue\n",
    "\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainset = open('process_1.txt', encoding='utf-8')     #读取训练集\n",
    "\n",
    "Init_Array()\n",
    "\n",
    "for line in trainset:  #对每行分别处理，添加状态序列、词典、状态转移概率和发射概率\n",
    "    line = line.strip()\n",
    "    line_num += 1\n",
    "\n",
    "    word_list = []\n",
    "    for k in range(len(line)):\n",
    "        if line[k] == ' ':continue\n",
    "        word_list.append(line[k])\n",
    "    if len(word_list) == 0:\n",
    "        continue\n",
    "    word_set = word_set | set(word_list)    #训练集所有字的集合\n",
    "\n",
    "    line = line.split()\n",
    "    line_state = []     #这句话的状态序列\n",
    "\n",
    "    for i in line:\n",
    "        line_state.extend(get_tag(i)) #获得并添加状态序列\n",
    "    array_Pi[line_state[0]] += 1  # array_Pi用于计算初始状态分布概率\n",
    "\n",
    "    for j in range(len(line_state)-1):\n",
    "        array_A[line_state[j]][line_state[j+1]] += 1  #array_A计算状态转移概率\n",
    "\n",
    "    for p in range(len(line_state)):\n",
    "        count_dic[line_state[p]] += 1  # 记录每一个状态的出现次数\n",
    "        \n",
    "        for state in STATES:\n",
    "            if word_list[p] not in array_B[state]:\n",
    "                array_B[state][word_list[p]] = 0.0  #保证每个字都在STATES的字典中\n",
    "        array_B[line_state[p]][word_list[p]] += 1  # array_B用于计算发射概率\n",
    "\n",
    "Prob_Array()    #对概率取对数保证精度\n",
    "\n",
    "\n",
    "testset = open('test_endfile.txt', encoding='utf-8')       #读取测试集\n",
    "outputfile = open('HMM分词.txt', mode='w', encoding='utf-8')\n",
    "output = ''\n",
    "\n",
    "for line in testset:\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    tag = Viterbi(line, array_Pi, array_A, array_B)\n",
    "    seg = tag_seg(line, tag)\n",
    "    list = ''\n",
    "    for i in range(len(seg)):\n",
    "        list = list + seg[i] + '/'\n",
    "    output = output + list + '\\n'\n",
    "    outputfile.write(list + '\\n')\n",
    "testset.close()\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
